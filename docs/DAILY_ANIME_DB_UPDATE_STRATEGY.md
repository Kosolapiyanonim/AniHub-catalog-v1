# Ежедневное обновление базы аниме: анализ текущего процесса и стратегия

## 1) Как сейчас устроен поиск и обновление базы

### Источник данных
- Основной источник — `Kodik API` (`https://kodikapi.com/list`) с параметрами:
  - `types=anime,anime-serial`
  - `with_material_data=true`
  - `limit=100`
- Для полной синхронизации используются последовательные страницы через `next_page`.

### Текущие API-пайплайны

#### `POST /api/parse-single-page`
- Парсит **одну страницу** Kodik (или `nextPageUrl`, если передан).
- Фильтрует записи без `shikimori_id`.
- Делает дедупликацию по `shikimori_id` внутри страницы.
- Трансформирует данные в формат `animes`.
- Выполняет `upsert` в `animes` по конфликту `shikimori_id`.
- Затем обрабатывает связи (жанры/студии/страны) и озвучки в `translations`.
- Используется из админки `/admin/parser` в цикле по страницам.

#### `POST /api/parse-latest`
- Инкрементальный режим: берет только страницу последних обновлений
  (`sort=updated_at&order=desc`).
- Делает `upsert` в `animes` и обработку связей.
- Быстрый путь для частого обновления.

#### `POST /api/parser`
- Обходит несколько страниц (`pagesToParse`) и пакетно обновляет:
  - `animes`
  - `genres`/`studios`/`countries`
  - `anime_genres`/`anime_studios`/`anime_countries`
  - `translations`
- Логирует статистику «добавлено/обновлено».

#### `POST /api/full-parser`
- Полная синхронизация (по умолчанию `maxPages=10`, можно больше).
- Использует `service_role` ключ Supabase.
- Также идет по страницам Kodik и делает пакетный `upsert`.

### Что уже хорошо
- Используется `upsert` (идемпотентность на уровне записи).
- Есть дедупликация по `shikimori_id` внутри батча.
- Есть индекс по `updated_at_kodik` для быстрых выборок по «свежести».
- Есть разделение на полный и инкрементальный режимы.

### Текущие ограничения / риски
- Нет автоматического расписания (cron) в репозитории.
- Админский сценарий завязан на ручной запуск страницы `/admin/parser`.
- Нет явного «курсора синхронизации» (`last_synced_at`) в БД.
- Частично дублируется логика парсинга между несколькими роутами.
- Нет централизованного контроля retry/backoff/метрик для продового ежедневного джоба.

---

## 2) Лучшие варианты ежедневного и эффективного обновления

Ниже — рекомендуемая схема в порядке приоритета.

### Вариант A (рекомендуемый): гибрид «частый инкремент + ночной догоняющий проход»

#### Идея
1. **Инкрементальный cron** каждые 1–3 часа:
   - вызывает `/api/parse-latest`
   - обрабатывает только самые свежие изменения
2. **Ночной job** 1 раз в сутки:
   - проходит `N` страниц (например, 20–100) через унифицированный endpoint
   - догоняет возможные пропуски и редкие изменения

#### Почему это оптимально
- Обновления появляются быстро (почти realtime для каталога).
- Ночная задача выравнивает консистентность и «страхует» от выпадений.
- Нагрузка распределяется, нет одного тяжелого гигантского запуска.

### Вариант B: только ежедневный полный прогон
- Запуск 1 раз/сутки большого `full-parser`.
- Плюс: просто.
- Минус: данные в течение дня устаревают, большие пики нагрузки.

### Вариант C: только частый `parse-latest`
- Очень легкий и частый.
- Минус: можно пропустить редкие кейсы, если изменения ушли глубже, чем 1 страница выдачи.

---

## Техническая реализация (практический план)

### Шаг 1. Ввести таблицу состояния синхронизации
Создать таблицу, например `sync_state`:
- `source` (`kodik`)
- `last_success_at`
- `last_cursor_url` (опционально)
- `last_run_stats` (jsonb)

Зачем:
- фиксировать последнюю успешную точку,
- делать рестарт без потери прогресса,
- хранить метрики и диагностику.

### Шаг 2. Унифицировать парсинг в один сервис
Свести повторяющуюся логику `/api/parser`, `/api/full-parser`, `/api/parse-single-page`, `/api/parse-latest` в один модуль, где параметры управляют режимом:
- `mode=latest | pages | full`
- `maxPages`
- `startCursor`

Это снизит риск расхождений и упростит поддержку.

### Шаг 3. Добавить cron
Если деплой на Vercel:
- добавить `crons` в `vercel.json`:
  - `0 */3 * * *` → `/api/parse-latest`
  - `30 2 * * *` → `/api/parser?pagesToParse=50` (пример)

Важно:
- закрыть endpoint секретом (`Authorization: Bearer ...`),
- отклонять вызовы без секрета.

### Шаг 4. Идемпотентность + блокировка параллельных запусков
- В начале job брать advisory lock (или запись в `sync_state` как mutex).
- Если lock занят — завершать job корректно (skip).
- Это предотвратит одновременные запуски и гонки.

### Шаг 5. Retry / backoff / time budget
- При `429/5xx` от Kodik: экспоненциальный backoff.
- Лимит по времени выполнения одного job.
- Сохранение промежуточного курсора каждые X страниц.

### Шаг 6. Очистка/консистентность связей
Сейчас связи в основном дописываются/upsert-ятся. Для полной консистентности периодически (например, в ночном джобе):
- пересобирать связи для затронутых аниме,
- удалять устаревшие `anime_genres`/`anime_studios`/`anime_countries`, которых уже нет в источнике.

### Шаг 7. Наблюдаемость
Логировать и сохранять метрики:
- сколько записей прочитано/обновлено/добавлено,
- сколько ошибок,
- длительность,
- lag от `updated_at_kodik` до времени синхронизации.

Пороговые алерты:
- job не запускался > 24ч,
- error-rate > X%,
- processed=0 несколько запусков подряд.

---

## Рекомендуемый SLA
- Инкрементальные обновления: каждые 1–3 часа.
- Доступность данных: 99% обновлений попадают в каталог в пределах 3 часов.
- Ночной reconcile: гарантирует целостность в пределах 24 часов.

---

## Минимальный MVP (быстрый запуск)
1. Добавить cron на `/api/parse-latest` каждые 3 часа.
2. Добавить cron ночного прохода 30–50 страниц.
3. Добавить проверку секрета для cron-endpoint.
4. Добавить простую таблицу `sync_state` и записывать итоги запусков.

Такой MVP уже даст автоматическое ежедневное обновление и заметный рост надежности.
