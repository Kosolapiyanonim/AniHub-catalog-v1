# Ежедневное обновление контента из Kodik API

## Почему полный парсинг неэффективен

Текущий полный парсинг (`/api/full-parser` и `/api/parser`) проходит по всем страницам Kodik API и заново обрабатывает всю базу. Для ежедневной синхронизации это даёт:
- избыточные запросы к Kodik;
- лишние upsert-операции в Supabase;
- долгие окна обновления;
- повышенный риск блокировок/лимитов.

## Что в Kodik API можно использовать для инкрементального обновления

Из локальной документации (`document api kodik.txt`) видно, что для `/list` есть:
- `sort=updated_at`;
- `order=desc`;
- пагинация через `next_page`;
- поле `updated_at` в каждом материале.

Это позволяет строить «водяной знак» (watermark) и не сканировать старые страницы.

## Рекомендуемая стратегия (production)

1. Брать последний `updated_at_kodik` из таблицы `animes`.
2. Вычитать safety window (например, 2 часа), чтобы не потерять обновления из-за рассинхрона времени/переиндексации.
3. Запрашивать `/list` с `sort=updated_at&order=desc&limit=100`.
4. Идти по `next_page`, пока материалы новее watermark.
5. Как только на странице начинают преобладать записи старее watermark — останавливать обход.
6. Делать upsert по `shikimori_id` и обновлять связанные таблицы (genres/studios/countries/translations).
7. Запускать это по cron 1 раз в сутки (или чаще, например каждые 6 часов).

## Что уже реализовано в проекте

Добавлен endpoint `/api/parse-daily` с инкрементальной логикой:
- авторизация через `CRON_SECRET` (если задан);
- watermark из `animes.updated_at_kodik`;
- safety window = 120 минут;
- остановка по достижении watermark;
- ограничение `maxPages` (по умолчанию 30);
- upsert аниме и связей через существующие parser-utils.

Также настроен Vercel Cron:
- `15 3 * * *` → `/api/parse-daily`.

## Параметры для тюнинга

`POST /api/parse-daily` поддерживает body:

```json
{
  "maxPages": 30,
  "lookbackHours": 24
}
```

- `maxPages`: защита от слишком долгого прогона;
- `lookbackHours`: fallback-окно, если база пустая или нет watermark.

## Дополнительные улучшения (следующий этап)

1. **Таблица состояния sync_job_state**
   - хранить `last_success_at`, `last_watermark`, `items_processed`, `duration_ms`, `status`.
2. **Очередь батчей на связи**
   - уменьшить время обновления жанров/студий при больших апдейтах.
3. **Алертинг**
   - отправка в Telegram/Slack при `processed=0` несколько дней подряд или при ошибках.
4. **Гибридный режим**
   - ежедневный инкремент + еженедельный полный reconcile (ночью), чтобы ловить редкие рассинхроны.

## Почему это наиболее эффективно

- Сканируется только «голова» каталога с новыми изменениями.
- Количество API-запросов и DB-upsert резко меньше, чем у full-scan.
- Есть защита от пропусков благодаря safety window.
- Легко масштабируется частотой cron без взрывного роста нагрузки.
